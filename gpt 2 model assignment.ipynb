{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Jaikishan Mohanty, Roll No: 2K22/SWE/07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Assignment: Implementation and Optimization of GPT-2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Task 1: Model Implementation and Checkpoints\n",
    "- Task 2: Architectural changes\n",
    "- Task 3: Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lLa0SZGAi00j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: rotary-embedding-torch in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: einops>=0.7 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from rotary-embedding-torch) (0.7.0)\n",
      "Requirement already satisfied: beartype in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from rotary-embedding-torch) (0.16.4)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from rotary-embedding-torch) (2.1.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (2023.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0->rotary-embedding-torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0->rotary-embedding-torch) (1.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: axial-positional-embedding in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from axial-positional-embedding) (2.1.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (4.3.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (2023.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from torch->axial-positional-embedding) (1.10.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from jinja2->torch->axial-positional-embedding) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from sympy->torch->axial-positional-embedding) (1.2.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (4.36.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jaikishan mohanty\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install rotary-embedding-torch\n",
    "!pip install torch\n",
    "!pip install axial-positional-embedding\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JtuKWnvri-6x"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: GPT-2 Model & Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V28PDCsRf1VP",
    "outputId": "52e2c28f-1b72-49c8-91ba-ef41aa727420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20, 128])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GPT2Small(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, heads, depth):\n",
    "        super(GPT2Small, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_embedding = nn.Embedding(1000, embed_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        MultiHeadAttention(embed_size, heads),\n",
    "                        nn.LayerNorm(embed_size),\n",
    "                        nn.Linear(embed_size, 4 * embed_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(4 * embed_size, embed_size),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
    "        out = self.word_embedding(x) + self.positional_embedding(positions)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            attention = layer[0](out, out, out, mask)\n",
    "            out = out + attention\n",
    "            out = layer[1](out)\n",
    "            out = layer[4](layer[3](layer[2](out)))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "embed_size = 128\n",
    "heads = 4\n",
    "depth = 3\n",
    "model = GPT2Small(vocab_size, embed_size, heads, depth)\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (32, 20))\n",
    "mask = (input_sequence != 0).unsqueeze(1).unsqueeze(2)\n",
    "output = model(input_sequence, mask)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Transformer Architectural Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Nt3t3sFgeEU",
    "outputId": "97ca0b0d-7d6a-4f47-ae1f-788a302e7da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaikishan Mohanty\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jaikishan Mohanty\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Once upon a time, in a way, I felt like I was in the middle of something.\n",
      "\n",
      "\"It was like, 'Oh my God, this is going to be great.' And then I realized that I wasn't going anywhere.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_text = \"Once upon a time, in a\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    attention_mask=torch.ones_like(input_ids)\n",
    ")\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Training Loop Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36bmSY5miGat",
    "outputId": "9e23f548-9c40-4877-e120-81a72fd455b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jaikishan Mohanty\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1:\n",
      " Input Prompt: Once upon a time, in a \n",
      "Generated Text: Once upon a time, in a time of war, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time of peace, in a time\n",
      "==================================================\n",
      "Test Case 2:\n",
      " Input Prompt: In a galaxy far, far away, \n",
      "Generated Text: In a galaxy far, far away, there is a galaxy far, far, far away.\n",
      "\n",
      "In a galaxy far, far away, there is a galaxy far, far, far, far, far, far, far, far, far\n",
      "==================================================\n",
      "Test Case 3:\n",
      " Input Prompt: To be or not to be, that is \n",
      "Generated Text: To be or not to be, that is not the point.\n",
      "\n",
      "The point is that if you want to be or not to be, that is not the point.\n",
      "\n",
      "The point is that if you want to be or not to be\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def test_gpt2_small(model, tokenizer, input_text, max_length=50, num_beams=5, temperature=0.7):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones_like(input_ids)\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "input_prompt = \"Once upon a time, in a\"\n",
    "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
    "print(\"Test Case 1:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "input_prompt = \"In a galaxy far, far away,\"\n",
    "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
    "print(\"Test Case 2:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "input_prompt = \"To be or not to be, that is\"\n",
    "output_text = test_gpt2_small(gpt2_model, tokenizer, input_prompt)\n",
    "print(\"Test Case 3:\\n\", \"Input Prompt:\", input_prompt, \"\\nGenerated Text:\", output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
